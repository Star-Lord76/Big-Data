{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DbLoadUtils import getMongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MongoDB successful\n"
     ]
    }
   ],
   "source": [
    "uri = \"mongodb+srv://admin:admin@bigdata.em7viry.mongodb.net/?retryWrites=true&w=majority&appName=BigData\"\n",
    "mongoClient = getMongoClient(uri)\n",
    "\n",
    "db = mongoClient[\"BigDataProj\"]\n",
    "collection = db[\"BigDataProj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Nico\\Documenti\\GitHub\\Big-Data\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "\n",
    "vectorStore = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    uri,\n",
    "    db.name + \".\" + collection.name,\n",
    "    embedding_model,\n",
    "    relevance_score_fn = \"cosine\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bb5f3de0b94f00b4f69cb599913b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "modelName = \"google/gemma-1.1-7b-it\"\n",
    "\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id=modelName,\n",
    "    model_kwargs={\"temperature\":0.1, \"max_length\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    if not text.strip():\n",
    "        print(\"Attempted to get embedding for empty text.\")\n",
    "        return []\n",
    "\n",
    "    embedding = embedding_model.embed_query(text)\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"default\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 10000,  \n",
    "                \"limit\": 10,  \n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  \n",
    "                \"utente\": 1,  \n",
    "                \"titolo\": 1,  \n",
    "                \"dottore\": 1,  \n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"},  \n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPrompt(query: str):\n",
    "    \n",
    "    docs = vector_search(query, collection=collection)\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc[\"dottore\"].strip() + \"\\n\\n\"\n",
    "        \n",
    "    istruction = \"\"\"Sei un dottore che deve rispondere alle domande di un paziente. Unisci la tua conoscenza pregressa a queste risposte fornite da medici ad altri pazienti con problemi simili ma non citarle direttamente. \n",
    "Non inventare. Genera una risposta rapida e concisa, senza ripetizioni. Usa un tono professionale e senza errori grammaticali. Indica unicamente la riposta alla domanda.\n",
    "Non rispondere con il tuo nome e non identificarti. Elenca delle possibili soluzione.\"\"\"\n",
    "    \n",
    "    return f\"\"\"CONTESTO: {context}\n",
    "DOMANDA: {query}\n",
    "ISTRUZIONI: {istruction}\n",
    "RISPOSTA:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(query: str):\n",
    "    prompt  = createPrompt(query)\n",
    "    response = hf.generate([prompt], max_new_tokens=300, do_sample = True)\n",
    "    splitted = response.generations[0][0].text.split(\"RISPOSTA:\")[-1]\n",
    "    return splitted.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per capire se sei celiaco, è importante sottoporsi ad una prova serologica specifica per la celiachia, come i anticorpi anti-gliadina. Inoltre, una gastroscopia con biopsie del duodeno può essere utilizzata per studiare i villi e la presenza di eventuali danni.\n"
     ]
    }
   ],
   "source": [
    "print(answer(\"Ciao, cosa devo fare per capire se sono celiaco?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=answer,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
